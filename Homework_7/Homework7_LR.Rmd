---
title: 'DS 624: Homwork 7 (Linear Regression and Cousins)'
subtitle: 'Kuhn: 6.2, 6.3'
author: 'Donny Lofland'
data: '10/31/2020'
output:
  html_document:
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float: true
  pdf_document:
    extra_dependencies: ["geometry", "multicol", "multirow", "xcolor"]
---

Source Code: [https://github.com/djlofland/DATA624_PredictiveAnalytics/tree/master/Homework_7](https://github.com/djlofland/DATA624_PredictiveAnalytics/tree/master/Homework_7)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(fpp2)
library(ggplot2)
library(tseries)
library(gridExtra)
library(forecast)
library(timetk)
library(tidyverse)
library(seasonal)
library(corrplot)
library(RColorBrewer)
library(lattice)
library(hablar)
library(naniar)

library(AppliedPredictiveModeling)
library(e1071)
library(mlbench)
library(caret)
library(ipred)
library(MASS)
library(elasticnet)
library(lars)
library(pls)
library(stats)
library(bnstruct)

library(outlieR)
library(outliers)

set.seed(424242)
```

## Problem 6.2

Developing a model to predict permeability (see Sect. 1.4) could save significant
resources for a pharmaceutical company, while at the same time more
rapidly identifying molecules that have a sufficient permeability to become a
drug:

a. Start R and use these commands to load the data:
```
library(AppliedPredictiveModeling)
data(permeability)
```
The matrix `fingerprints` contains the 1,107 binary molecular predictors
for the 165 compounds, while `permeability` contains permeability
response.

```{r}
data("permeability")

fp <- fingerprints
pm <- permeability

print(paste(nrow(fp), ncol(fp)))
print(paste(nrow(pm), ncol(pm)))
```

```{r echo=FALSE}
# Check for missing values
missing <- colSums(as.data.frame(fp) %>% sapply(is.na))
missing_pct <- round(missing / nrow(fp) * 100, 2)

if(length(missing_pct[missing_pct > 0])) {
  stack(sort(missing_pct[missing_pct > 0], decreasing = TRUE))
}


missing <- colSums(as.data.frame(pm) %>% sapply(is.na))
missing_pct <- round(missing / nrow(pm) * 100, 2)

if(length(missing_pct[missing_pct > 0])) {
  stack(sort(missing_pct[missing_pct > 0], decreasing = TRUE))
}
```

> Note: no missing values found

b. The fingerprint predictors indicate the presence or absence of substructures
of a molecule and are often sparse meaning that relatively few of the
molecules contain each substructure. Filter out the predictors that have
low frequencies using the `nearZeroVar` function from the caret package.
How many predictors are left for modeling?

```{r}
nz <- nearZeroVar(fp)
fp <- fingerprints[,-nz]

print(paste(nrow(fp), ncol(fp)))
```

> We have 388 predictors left for predicting.

c. Split the data into a training and a test set, pre-process the data, and
tune a PLS model. How many latent variables are optimal and what is
the corresponding resampled estimate of $R^2$?

```{r}
set.seed(424242)

# get training/test split
trainingRows <- createDataPartition(pm, p=0.8, list=FALSE)

# Build training datasets
trainX <- fp[trainingRows,]
trainY <- pm[trainingRows]

# put remaining rows into the test sets
testX <- fp[-trainingRows,]
testY <- pm[-trainingRows]

# Build a DF
trainingData <- as.data.frame(trainX)
trainingData$Perm <- trainY

ctrl <- trainControl(method='cv', number=10)

# run Cv to identify # of latent components
plsTune <- train(trainX, trainY,
                 method="pls",
                 tuneLength=20,
                 trControl = ctrl)
plot(plsTune)

# Get the number of components which minimizes RMSE
(optimal_num_comp <- which.min(plsTune$results[,2]))
(optimal_rmse <- plsTune$results[optimal_num_comp,3])
```

> I found the optimal number of PLS latent components to be `r optimal_num_comp` giving and RMSE of `r optimal_rmse`.

d. Predict the response for the test set. What is the test set estimate of $R^2$?

```{r}
plsFit <- plsr(Perm ~., data=trainingData, ncomp=optimal_num_comp)

plsPred <- predict(plsFit, testX, ncomp=optimal_num_comp)
head(plsPred)

plsValues <- data.frame(obs = testY, pred=plsPred)
colnames(plsValues) = c('obs', 'pred')
defaultSummary(plsValues)
```

> $R^2$ for the test set is 0.4068

e. Try building other models discussed in this chapter. Do any have better
predictive performance?

```{r}
# Ridge Model
set.seed(100)

ridgeGrid <- data.frame(.lambda=seq(0, 0.2, length=21))

ctrl <- trainControl(method='cv', number=10)

ridgeRegFit <- train(trainX, trainY,
                     method="ridge", 
                     tuneGrid = ridgeGrid,
                     trControl = ctrl)

(ridgeRegFit)

# Do prediction and get R^2
ridgePred <- predict(ridgeRegFit, newdata=testX)

ridgeValues <- data.frame(obs = testY, pred=ridgePred)
colnames(ridgeValues) = c('obs', 'pred')
defaultSummary(ridgeValues)
```

```{r}

```

```{r}
set.seed(100)

# LASSO
enetGrid <- expand.grid(.lambda=c(0, 0.01, 0.1),
                        .fraction=seq(0.05, 1, length = 20))

ctrl <- trainControl(method='cv', number=10)

enetTune <- train(trainX, trainY,
                     method="enet", 
                     tuneGrid = enetGrid,
                     trControl = ctrl)
plot(enetTune)
summary(enetTune)

lassoPred <- predict(enetTune, newdata=testX)

lassoValues <- data.frame(obs = testY, pred=lassoPred)
colnames(lassoValues) = c('obs', 'pred')
defaultSummary(lassoValues)
```


f. Would you recommend any of your models to replace the permeability laboratory experiment?

> I would NOT recommend replacement of laboratory experiments, but possibly a model could be used to help prioritize the order of research.  Then based on business needs, if it made sense to stop early, we are more likely to have found those that are most meaningful.  Between the couple of models, the Ridge model explains ~40% of the variability in permability based on features which may or may not be sufficient to justify altering processes.  

## Problem 6.3

A chemical manufacturing process for a pharmaceutical product was
discussed in Sect. 1.4. In this problem, the objective is to understand the relationship
between biological measurements of the raw materials (predictors),
measurements of the manufacturing process (predictors), and the response of
product yield. Biological predictors cannot be changed but can be used to
assess the quality of the raw material before processing. On the other hand,
manufacturing process predictors can be changed in the manufacturing process.
Improving product yield by 1% will boost revenue by approximately
one hundred thousand dollars per batch:

a. Start R and use these commands to load the data:
```
library(AppliedPredictiveModeling)
data(chemicalManufacturing)
```
The matrix `processPredictors` contains the 57 predictors (12 describing
the input biological material and 45 describing the process predictors)
for the 176 manufacturing runs. `yield` contains the percent yield for each
run.

```{r}
data("ChemicalManufacturingProcess")

cmp <- as_tibble(ChemicalManufacturingProcess)

x_raw <- cmp[,2:58]
y_raw <- as.matrix(cmp$Yield)

print(paste(nrow(x_raw), ncol(x_raw)))
print(paste(nrow(y_raw), ncol(y_raw)))
```

b. A small percentage of cells in the predictor set contain missing values. Use
an imputation function to fill in these missing values (e.g., see Sect. 3.8).

```{r}
# Various NA plots to inspect data
knitr::kable(miss_var_summary(cmp) %>% filter(n_miss > 0), 
             caption = 'Missing Values',
             format="html", 
             table.attr="style='width:50%;'") %>% 
  kableExtra::kable_styling()
gg_miss_var(cmp)
gg_miss_upset(cmp)
```

> Because we see some patterns in how features are missing, I'll use a KNN Impute approach.  Note that the`caret` `knnImpute` also normalizes the data by default.  This is annoying - I'm going use `knn.impute` from `bnstruct` instead.

```{r}
x_imputed <- knn.impute(as.matrix(x_raw), k=10)
```

c. Split the data into a training and a test set, pre-process the data, and
tune a model of your choice from this chapter. What is the optimal value
of the performance metric?

Let's check for and drop any features with near zero variance

```{r}
# Check for columns with little variance - candidate to drop
lowVariance <- nearZeroVar(x_imputed, names = TRUE)
head(lowVariance)
lowVariance <- nearZeroVar(x_imputed)
```

````{r}
# Drop columns with low variance
x_lowvar <- x_imputed[,-lowVariance]
```

Let's deal with outliers - rrepalce them with the MEDIAN from that feature:

```{r}
x_outliers <- outlieR::impute(x_lowvar, fill='median')
```


Now, lets drop any columns with high correlations

```{r}
# Find and drop high correlation features
correlations <- cor(x_outliers)
highCorr <- findCorrelation(correlations, names=TRUE, cutoff=0.9)
(highCorr)

highCorr <- findCorrelation(correlations, cutoff=0.9)
x_corr <- x_outliers[,-highCorr]

head(x_corr)
```

Now, let's transform our features (scale, center and boxcox) to improve modeling resolution

```{r}
x_transf <-  preProcess(x_corr, method=c('center', 'scale', 'BoxCox'))
(x_transf)
x_transf <- predict(x_transf, x_corr)
```

Time to split in to Training and Test

```{r}

# get training/test split
trainingRows <- createDataPartition(y_raw, p=0.8, list=FALSE)

# Build training datasets
trainX <- x_transf[trainingRows,]
trainY <- y_raw[trainingRows]

# put remaining rows into the test sets
testX <- x_transf[-trainingRows,]
testY <- y_raw[-trainingRows]

# Build a DF
trainingData <- as.data.frame(trainX)
trainingData$Yield <- trainY


```

Build a model:

```{r}
set.seed(100)

# LASSO
enetGrid <- expand.grid(.lambda=c(0, 0.01, 0.1),
                        .fraction=seq(0.05, 1, length = 20))

ctrl <- trainControl(method='cv', number=10)

enetTune <- train(trainX, trainY,
                     method="enet", 
                     tuneGrid = enetGrid,
                     trControl = ctrl)
plot(enetTune)
summary(enetTune)

lassoPred <- predict(enetTune, newdata=trainX)

lassoValues <- data.frame(obs = trainY, pred=lassoPred)
colnames(lassoValues) = c('obs', 'pred')
(train_values <- defaultSummary(lassoValues))
```

d. Predict the response for the test set. What is the value of the performance
metric and how does this compare with the resampled performance metric
on the training set?

```{r}
lassoPred <- predict(enetTune, newdata=testX)

lassoValues <- data.frame(obs = testY, pred=lassoPred)
colnames(lassoValues) = c('obs', 'pred')
(test_values <- defaultSummary(lassoValues))
```

> With the training data, we had an RMSE of `r train_values[2]`.  With the test data, this was `r test_values[2]`.  These seem reasonable and quite good.  We expect the training RMSE to be higher.  

e. Which predictors are most important in the model you have trained? Do
either the biological or process predictors dominate the list?

```{r}
enetImp <- varImp(enetTune, scale = FALSE)
plot(enetImp, top = 20)
```

> We have more Manufacturing Features in the top 20 (15) vs Biological (5).  That said, we had more Manufacturing features to start with so, this could just be a class imbalance issue. ... or maybe Manufacturing Processes have more impact than base Biological?

f. Explore the relationships between each of the top predictors and the response.
How could this information be helpful in improving yield in future
runs of the manufacturing process?

```{r fig.height=12, fig.width=8, warning=FALSE}
# Get top feature names
features <- as.data.frame(enetImp[["importance"]]) %>%
  arrange(-Overall, ) %>%
  rownames()

features <- features[1:20]

cmp_imp <- as.data.frame(x_outliers)[features]
cmp_imp$Yield <- cmp$Yield

featurePlot(x = cmp_imp[features], 
            y = cmp_imp$Yield, 
            plot = "scatter",
            type = c("p", 'smooth'),
            span = .5,
            pch = 20)
# Show feature correlations/target by decreasing correlation
#stack(sort(cor(clean_df[,1], clean_df[,2:ncol(clean_df)])[,], decreasing=TRUE))
```

> Since we cannot affect Biological Features, we can ignore those for now.  However, for the Manufacuring features, we can look for those that show a relationship and try altering our Manufacturing towards the more favorable values.  At this point, we haven't established causation, merely correlation.  So, changing those values might not lead to improvements, however these are good candidates for exploration.  I would setup experimets where we independently test changing each candidate Manufacturing feature with the strongest correlation and starting from the top of list (based on feature importance) to see if it's a lever for improving yield. Note: Just because a variable appeared at the top of the feature importance doesn't mean it is the lever that could have the most impact if changed ... but it's a good place to start and better than just randomly adjusting manufactuing levers.
