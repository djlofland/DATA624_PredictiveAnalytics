---
title: 'DS 624: Homwork 10 (Market Basket Analysis)'
subtitle: 'Analyze Grocery Store Receipts'
author: 'Donny Lofland'
data: '10/31/2020'
output:
  html_document:
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float: true
  pdf_document:
    extra_dependencies: ["geometry", "multicol", "multirow", "xcolor"]
---

Source Code: [https://github.com/djlofland/DATA624_PredictiveAnalytics/tree/master/Homework_10](https://github.com/djlofland/DATA624_PredictiveAnalytics/tree/master/Homework_10)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Imagine 10000 receipts sitting on your table. Each receipt represents a transaction with items that were purchased. The receipt is a representation of stuff that went into a customer’s basket - and therefore ‘Market Basket Analysis’.

That is exactly what the Groceries Data Set contains: a collection of receipts with each line representing 1 receipt and the items purchased. Each line is called a transaction and each column in a row represents an item.  The data set is attached.

Your assignment is to use R to mine the data for association rules. You should report support, confidence and lift and your top 10 rules by lift. 

Extra credit: do a simple cluster analysis on the data as well.  Use whichever packages you like.  Due May 3 before midnight.


```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
# Load the libraries
library(arules)
library(arulesViz)
library(datasets)

library(tidyverse)
library(reshape2)
library(RColorBrewer)

options(digits=2)
set.seed(424242)
```

## Load Data

```{r}
# Load Groceries Dataset
groceries_df <- read.csv("./GroceryDataSet.csv", header = FALSE,
                         na.strings="", 
                         stringsAsFactors=FALSE )

# Add an id column
groceries_df$id <- seq(nrow(groceries_df))

# Trim any white space at the begining or end of product names
groceries_df <- groceries_df %>%
  mutate(across(where(is.character), str_trim))

#cols_to_trim <- names(groceries_df)[vapply(groceries_df, is.character, logical(1))]
#groceries_df[,cols_to_trim] <- lapply(groceries_df[,cols_to_trim], trimws)

# How many baskets and max products?
print(paste(nrow(groceries_df), ncol(groceries_df)))

# Since the data.frame has empty cells, we need to clean those out
groceries_long <- melt(groceries_df, id.vars="id")
groceries_trans <- as(lapply(split(groceries_long$value, groceries_long$id), unique), "transactions")

# View first few baskets to make sure things look good
inspect(groceries_trans[1:5])

# Plot product frequency for top items

itemFrequencyPlot(groceries_trans, 
                  topN=30, 
                  type="absolute",
                  col=brewer.pal(8,'Pastel2'),
                  main='Absolute Item Frequency Plot',
                  ylab="Item Frequency (Absolute)")
# iFreq <- itemFrequency(groceries_trans, type="relative")
```

### Create Basket Rules

I am filtering to only include rules with 1~4 items on the RHS.  Rules with 5+ items are probably less useful and would increase noise.  Given the large number of receipts, I'm using a lower `support` threshold and I experimented with different `confidence` values to get what look like reasonable rules.

Note: I tried to remove redundant rules using examples from documentation, but for some reason (I don't know why), the filtering treated ALL rules as redundant.  After a fair bit of troubleshooting, I just removed that step for now.  This would certainly be something I would circle back to if I were using this analysis for real.

```{r}
rules <- apriori(groceries_trans, 
                 parameter = list(supp = 0.001, 
                                  conf = 0.15,
                                  minlen = 2,
                                  maxlen = 5))


# find redundant rules
# subset.matrix <- is.subset(rules, rules)
# subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA
# redundant <- colSums(subset.matrix, na.rm=T) >= 1

# remove redundant rules
# rules.pruned <- rules[!redundant]
# rules <- rules.pruned

rules <- sort(rules, by="lift", decreasing=TRUE)
inspect(rules[1:10])
```

Stepping back, if this were being used in a real setting, I would explore generating a `score` using a geometric mean of `support`, `confidence` and `lift` ... then sorting my rules based on this new metric, `score`.  Ideally we want rules that are both prevalent and meaningful.  A large `lift` with low `support` not be actionable. Alternatively, a high `confidence` with low `support` or low `lift` would also not be as actionable.  Ideally, we want rules that identify both high `lift` and high `support`.  

Per instructions, I've just sorted based on `lift` for now; however, `lift` alone probably isn't the best approach.

### Visualize Rules

```{r fig.height=6, fig.width=6, warning=F}
# Sort rules by Confidence
plot(rules[1:20], 
     method="graph", 
     engine='default', 
     control = list(type='items'))
```

```{r}
plot(rules[1:20],
     method = "paracoord",
     control = list(reorder = TRUE))
```

### Cluster Analysis

```{r fig.height=8, fig.width=8}
s <- groceries_trans[,itemFrequency(groceries_trans) > 0.02]
d_jaccard <- dissimilarity(s, which = "items", method="affinity")
plot(hclust(d_jaccard, method = "ward.D2"), main = "Dendrogram for Items")
```

```{r fig.height=8, fig.width=8}
d_affinity <- dissimilarity(rules[1:20], 
                            method = "affinity", 
                            args = list(transactions = groceries_trans))
hc <- hclust(d_affinity, method = "ward.D2")
plot(hc, main = "Dendrogram for Rules (Affinity)") 

## create 4 groups and inspect the rules in the first group.
assign <- cutree(hc, k = 3)
inspect(rules[assign == 1])
```
